\input{preamble.tex}
\textbf{\large Assignment 5: Panel Data I}\\\vspace{10pt}
\end{center}

\vspace{10pt}
\noindent
\textbf{\large Instructions:}

\vspace{10pt}
\begin{itemize}
\setlength\itemsep{0pt}
  \item {\color{red}{\textbf{Deadline}}}: \textbf{March 19, before class}
  \item Submit your work in a separate folder in your GitHub repository
  \begin{itemize}
    \item You can include only the R file or additional ones (e.g. pdf with results)
  \end{itemize}
  \item \textbf{Always use comments} in your R code -- and use them to answer questions
  \item You are encouraged to work together, but each person must submit their own code
  \item Plan is to start Part 1 in class and complete Part 2 at home
  \item I'll upload a solution file to the website after next class
\end{itemize}

\vspace{20pt}
\tableofcontents
\newpage

% ==========================================================================
\section{Part 1: In-Class (Presidential Approval)}
% ==========================================================================

In this lab, we analyze the relationship between unemployment and presidential approval ratings using U.S. state-level panel data. The goal is to understand how fixed effects models differ from pooled OLS and what each estimator actually identifies. Recall from the lecture that panel data allow us to control for unobserved time-invariant differences across units --- a key advantage over simple cross-sectional regressions.

Download the data here:
\begin{itemize}
  \item \href{https://github.com/franvillamil/AQM2/tree/master/datasets/presidential_approval}{github.com/franvillamil/AQM2/tree/master/datasets/presidential\_approval}
\end{itemize}

Load it with \code{readstata13::read.dta13("presidential\_approval.dta")}. Key variables:
\begin{itemize}
  \item \code{State} --- state name
  \item \code{StCode} --- state numeric ID
  \item \code{Year} --- year
  \item \code{PresApprov} --- percent positive presidential approval
  \item \code{UnemPct} --- state unemployment rate
  \item \code{South} --- southern state (1 = yes, 0 = no)
\end{itemize}

\subsection{Setup and data exploration}

\begin{enumerate}[label=\alph*)]
  \item Load the dataset. How many unique states and years are in the data? Use \code{length(unique())} or \code{n\_distinct()} to check. Is the panel balanced (i.e., does every state appear the same number of times)?
  \item Compute summary statistics for \code{PresApprov} and \code{UnemPct} using \code{summary()} or \code{modelsummary::datasummary\_skim()}. Then plot \code{PresApprov} over \code{Year} for a few selected states (e.g., California, Texas, New York) to visualize the panel structure:
  \begin{lstlisting}
library(dplyr)
library(ggplot2)

df_sub = df %>%
  filter(State %in% c("California", "Texas", "NewYork"))

ggplot(df_sub, aes(x = Year, y = PresApprov, color = State)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Year", y = "Presidential approval (%)", color = "State")
  \end{lstlisting}
  In a comment, describe the trends. Do states move together over time?
  \item Create a scatter plot of \code{PresApprov} (y-axis) against \code{UnemPct} (x-axis) across all state-year observations. Add a regression line with \code{geom\_smooth(method = "lm")}. In a comment, describe the cross-sectional relationship: does higher unemployment seem to be associated with lower or higher approval ratings?
\end{enumerate}

\subsection{Pooled OLS}

\begin{enumerate}[label=\alph*)]
  \item Estimate a pooled OLS model regressing presidential approval on unemployment: \\\code{m\_pooled = lm(PresApprov \textasciitilde{} UnemPct, data = df)}. Report the results using \code{summary()} or \code{modelsummary()}. In a comment, interpret the coefficient on \code{UnemPct}: what does it say about the relationship between unemployment and approval?
  \item Add \code{South} as a control: \\\code{m\_pooled2 = lm(PresApprov \textasciitilde{} UnemPct + South, data = df)}. Does controlling for whether a state is in the South change the coefficient on \code{UnemPct}? In a comment, explain why or why not.
  \item In a comment, reflect on the limitations of pooled OLS for this type of data. What kinds of unobserved, time-invariant differences across states might bias the estimate of the unemployment effect? Give two or three concrete examples.
\end{enumerate}

\subsection{Entity fixed effects}

\begin{enumerate}[label=\alph*)]
  \item Estimate a model with state fixed effects using \code{fixest}:
  \begin{lstlisting}
library(fixest)
m_fe = feols(PresApprov ~ UnemPct | State, data = df)
  \end{lstlisting}
  Report the results alongside the pooled OLS model in a single \code{modelsummary()} table. How does the coefficient on \code{UnemPct} change compared to pooled OLS?
  \item In a comment, explain what the state fixed effects are absorbing. Note that the \code{South} variable drops out of the model --- why can't it be estimated when state fixed effects are included? What does this imply about any variable that does not vary within a state over time?
  \item What does the coefficient on \code{UnemPct} now identify? In a comment, explain the intuition: the state FE estimator compares approval ratings \textit{within} the same state across different years, rather than across different states. How does this differ from the pooled OLS interpretation?
\end{enumerate}

\subsection{Two-way fixed effects}

\begin{enumerate}[label=\alph*)]
  \item Add year fixed effects to absorb common time shocks (e.g., national economic conditions, wars, presidential scandals) that affect all states simultaneously:
  \begin{lstlisting}
m_twfe = feols(PresApprov ~ UnemPct | State + Year, data = df)
  \end{lstlisting}
  \item Compare all three models in a single \code{modelsummary()} table with standard errors clustered by state:
  \begin{lstlisting}
modelsummary(
  list("Pooled OLS" = m_pooled, "State FE" = m_fe, "Two-Way FE" = m_twfe),
  vcov = ~State,
  stars = TRUE,
  gof_map = c("r.squared", "nobs"))
  \end{lstlisting}
  \item In a comment, discuss what the year fixed effects are controlling for. Does adding them change the coefficient on \code{UnemPct}? If so, what does that suggest about the role of common time trends in driving the relationship between unemployment and approval?
\end{enumerate}

\newpage

% ==========================================================================
\section{Part 2: Take-Home (Teaching Evaluations)}
% ==========================================================================

We now turn to a classic question in higher education research: does giving higher grades lead to better teaching evaluations? This is known as the ``grade inflation'' or ``grading leniency'' problem. Using instructor-level panel data, we will estimate whether instructors who give more A grades receive higher student evaluations --- and whether fixed effects change our conclusions.

Download the data here:
\begin{itemize}
  \item \href{https://github.com/franvillamil/AQM2/tree/master/datasets/teaching_evals}{github.com/franvillamil/AQM2/tree/master/datasets/teaching\_evals}
\end{itemize}

Load it with \code{readstata13::read.dta13("teaching\_evals.dta")}. Key variables:
\begin{itemize}
  \item \code{Eval} --- average course evaluation (5-point scale)
  \item \code{Apct} --- percent of students receiving an A or A- in the course
  \item \code{Enrollment} --- number of students enrolled
  \item \code{Required} --- dummy variable: 1 if the course is required
  \item \code{InstrID} --- unique identifier for each instructor
  \item \code{CourseID} --- unique identifier for each course
  \item \code{Year} --- academic year
\end{itemize}

\subsection{Data exploration}

\begin{enumerate}[label=\alph*)]
  \item How many unique instructors and courses are in the data? Use \code{n\_distinct(df\$InstrID)} and \code{n\_distinct(df\$CourseID)}. What is the average number of observations (course-year pairs) per instructor? In a comment, note whether this looks like a short or long panel.
  \item Create a scatter plot of \code{Eval} (y-axis) against \code{Apct} (x-axis). Add a regression line with \code{geom\_smooth(method = "lm")}. In a comment, describe the cross-sectional relationship between grading generosity and evaluations: is it positive or negative? Does this pattern surprise you?
\end{enumerate}

\subsection{Pooled OLS baseline}

\begin{enumerate}[label=\alph*)]
  \item Estimate a pooled OLS model with all three regressors: \\\code{m1 = lm(Eval \textasciitilde{} Apct + Enrollment + Required, data = df)}. Report the results using \code{summary()} or \code{modelsummary()}. In a comment, interpret the coefficient on \code{Apct}: a one-percentage-point increase in the share of A grades is associated with how much of a change in evaluation scores?
  \item In a comment, explain why the OLS estimate of \code{Apct} might be biased. What unobserved characteristics of instructors could simultaneously drive both grading generosity and evaluation scores? Give at least two concrete examples. Is the expected bias upward or downward?
\end{enumerate}

\subsection{Fixed effects models}

\begin{enumerate}[label=\alph*)]
  \item Estimate a model with instructor fixed effects, and a two-way model adding year fixed effects:
  \begin{lstlisting}
library(fixest)
m_instr = feols(Eval ~ Apct + Enrollment + Required | InstrID, data = df)
m_twfe  = feols(Eval ~ Apct + Enrollment + Required | InstrID + Year, data = df)
  \end{lstlisting}
  \item Compare all three models (\code{m1}, \code{m\_instr}, \code{m\_twfe}) in a single table with standard errors clustered by instructor:
  \begin{lstlisting}
modelsummary(
  list("Pooled OLS" = m1, "Instructor FE" = m_instr, "Two-Way FE" = m_twfe),
  vcov = ~InstrID,
  stars = TRUE,
  gof_map = c("r.squared", "nobs"))
  \end{lstlisting}
  \item Interpret the coefficient on \code{Apct} in the instructor-FE model (\code{m\_instr}). In a comment, explain what the instructor fixed effect is controlling for. Is the FE coefficient on \code{Apct} larger or smaller than in the pooled OLS? What does this tell us about the direction of omitted variable bias in the pooled OLS estimate --- are more lenient graders systematically better or worse evaluators in terms of their unobserved characteristics?
\end{enumerate}

\subsection{Random effects and the Hausman test}

\begin{enumerate}[label=\alph*)]
  \item Estimate a random effects model using \code{plm}. The random effects model assumes that the unobserved instructor-level heterogeneity is uncorrelated with the regressors:
  \begin{lstlisting}
library(plm)
pdata = pdata.frame(df, index = c("InstrID", "CourseID"))
m_re  = plm(Eval ~ Apct + Enrollment + Required,
            data = pdata, model = "random")
  \end{lstlisting}
  \item Run the Hausman test to assess whether fixed or random effects is more appropriate. The Hausman test checks whether the random effects assumption (no correlation between unobservables and regressors) holds:
  \begin{lstlisting}
m_fe_plm = plm(Eval ~ Apct + Enrollment + Required,
               data = pdata, model = "within")
phtest(m_fe_plm, m_re)
  \end{lstlisting}
  \item In a comment, interpret the Hausman test result. What is the null hypothesis? Is it rejected? Based on the test and on the substantive reasoning from the previous subsection, should you prefer the fixed effects or the random effects estimator for this dataset? Explain in 3--5 sentences.
\end{enumerate}

\vspace{15pt}

% ==========================================================================
\section{Data Sources}
% ==========================================================================

Both datasets are available at the course GitHub repository:
\begin{itemize}
  \item Presidential approval: \href{https://github.com/franvillamil/AQM2/tree/master/datasets/presidential_approval}{github.com/franvillamil/AQM2/tree/master/datasets/presidential\_approval}
  \item Teaching evaluations: \href{https://github.com/franvillamil/AQM2/tree/master/datasets/teaching_evals}{github.com/franvillamil/AQM2/tree/master/datasets/teaching\_evals}
\end{itemize}

\vspace{15pt}

% ==========================================================================
\section{Submission}
% ==========================================================================

Commit your file to your GitHub repository before the deadline. Put it in a separate folder, e.g.\ \texttt{assignment5}. Make sure your repository is public so I can access it.

Your R script should:
\begin{itemize}
  \item Be well-organized with clear section headers (using comments)
  \item Include all code needed to reproduce your analysis
  \item Include your answers and interpretations as comments
  \item Save any plots to files (e.g., using \code{ggsave()})
  \item Run without errors from top to bottom
\end{itemize}

\end{document}
