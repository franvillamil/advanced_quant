% ----------------------------------------------------
\begin{frame}
  \titlepage
\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Today's goals}
\centering

\begin{itemize}[<+->]
\item Learn to model conditional relationships (interactions)
\item Understand non-linear relationships
\item Review hypothesis testing and confidence intervals
\item Discuss how to present and interpret results
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ====================================================
\section{Interaction Effects}
% ====================================================

% ----------------------------------------------------
\begin{transitionframe}
Interaction Effects
\end{transitionframe}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{When effects depend on context}
\centering

\begin{itemize}[<+->]
\item Sometimes, the effect of $X$ on $Y$ depends on another variable $Z$
\item[]
\item Examples:
  \begin{itemize}
  \item Effect of education on income may differ by gender
  \item Effect of campaign spending may differ by incumbency status
  \item Effect of democracy on growth may depend on economic development
  \end{itemize}
\item[]
\item We model this with \textbf{interaction terms}
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{The interaction model}
\centering

\vspace{10pt}

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 (X \times Z) + \varepsilon$$

\vspace{20pt}

\begin{itemize}[<+->]
\item $\beta_1$: effect of $X$ when $Z = 0$
\item $\beta_2$: effect of $Z$ when $X = 0$
\item $\beta_3$: how the effect of $X$ changes as $Z$ increases
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{The marginal effect of $X$}
\centering

\vspace{10pt}

$$\frac{\partial Y}{\partial X} = \beta_1 + \beta_3 Z$$

\vspace{20pt}

\begin{itemize}[<+->]
\item The effect of $X$ is no longer a single number
\item It's a \textbf{function} of $Z$
\item[]
\item Need to report effects at meaningful values of $Z$
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Common mistakes with interactions}
\centering

\begin{itemize}[<+->]
\item \textbf{Mistake 1}: Interpreting $\beta_1$ as ``the effect of $X$''
  \begin{itemize}
  \item It's only the effect when $Z = 0$
  \item May not even be meaningful!
  \end{itemize}
\item[]
\item \textbf{Mistake 2}: Omitting constitutive terms
  \begin{itemize}
  \item Always include $X$ and $Z$ separately, not just $X \times Z$
  \end{itemize}
\item[]
\item \textbf{Mistake 3}: Not showing how the effect varies
  \begin{itemize}
  \item Plot the marginal effect across values of $Z$
  \end{itemize}
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Best practice: Visualize interactions}
\centering

\begin{itemize}[<+->]
\item Tables of coefficients are hard to interpret
\item[]
\item Better approach:
  \begin{itemize}
  \item Plot predicted values of $Y$ for different combinations of $X$ and $Z$
  \item Plot the marginal effect of $X$ across values of $Z$
  \item Include confidence intervals
  \end{itemize}
\item[]
\item Packages like \texttt{marginaleffects} make this easy in R
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ====================================================
\section{Non-linear Relationships}
% ====================================================

% ----------------------------------------------------
\begin{transitionframe}
Non-linear Relationships
\end{transitionframe}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{When linearity is not enough}
\centering

\begin{itemize}[<+->]
\item Linear regression assumes a constant slope
\item But relationships are often curved
\item[]
\item Examples:
  \begin{itemize}
  \item Diminishing returns (logarithmic)
  \item U-shaped or inverted U-shaped relationships
  \item Threshold effects
  \end{itemize}
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Polynomial terms}
\centering

\vspace{10pt}

$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \varepsilon$$

\vspace{20pt}

\begin{itemize}[<+->]
\item Adding $X^2$ allows the slope to change with $X$
\item Positive $\beta_2$: U-shaped relationship
\item Negative $\beta_2$: inverted U-shape
\item[]
\item Marginal effect: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2 X$
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Logarithmic transformations}
\centering

\begin{itemize}[<+->]
\item \textbf{Log-linear}: $Y = \beta_0 + \beta_1 \log(X)$
  \begin{itemize}
  \item Effect of $X$ decreases as $X$ increases (diminishing returns)
  \end{itemize}
\item[]
\item \textbf{Linear-log}: $\log(Y) = \beta_0 + \beta_1 X$
  \begin{itemize}
  \item A one-unit change in $X$ changes $Y$ by $(\exp(\beta_1)-1) \times 100\%$
  \end{itemize}
\item[]
\item \textbf{Log-log}: $\log(Y) = \beta_0 + \beta_1 \log(X)$
  \begin{itemize}
  \item $\beta_1$ is an elasticity: 1\% change in $X$ $\rightarrow$ $\beta_1$\% change in $Y$
  \end{itemize}
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{When to use transformations}
\centering

\begin{itemize}[<+->]
\item Logarithms are common for:
  \begin{itemize}
  \item Variables with skewed distributions (income, GDP, population)
  \item Multiplicative relationships
  \end{itemize}
\item[]
\item Polynomials are useful for:
  \begin{itemize}
  \item Relationships that curve in predictable ways
  \item Capturing turning points
  \end{itemize}
\item[]
\item Always check if the transformation makes sense substantively
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ====================================================
\section{Inference}
% ====================================================

% ----------------------------------------------------
\begin{transitionframe}
Inference: Standard Errors and Hypothesis Testing
\end{transitionframe}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{What are standard errors?}
\centering

\begin{itemize}[<+->]
\item Our estimates ($\hat{\beta}$) are based on a sample
\item A different sample would give different estimates
\item[]
\item The \textbf{standard error} measures this variability
\item It tells us how precise our estimate is
\item[]
\item Smaller SE = more confidence in our estimate
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{What affects standard errors?}
\centering

\begin{itemize}[<+->]
\item \textbf{Sample size}: More data $\rightarrow$ smaller SE
\item \textbf{Variation in $X$}: More spread in $X$ $\rightarrow$ smaller SE
\item \textbf{Error variance}: More noise in $Y$ $\rightarrow$ larger SE
\item[]
\item Intuition: We learn more from larger, more varied samples with less noise
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Confidence intervals}
\centering

\vspace{10pt}

$$\text{95\% CI} = \hat{\beta} \pm 1.96 \times SE(\hat{\beta})$$

\vspace{20pt}

\begin{itemize}[<+->]
\item Interpretation: If we repeated the study many times...
\item ...95\% of the CIs would contain the true $\beta$
\item[]
\item \textbf{Not}: ``There's a 95\% probability $\beta$ is in this interval''
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Hypothesis testing}
\centering

\begin{itemize}[<+->]
\item Null hypothesis: $H_0: \beta = 0$ (no effect)
\item Alternative: $H_1: \beta \neq 0$ (there is an effect)
\item[]
\item Test statistic: $t = \frac{\hat{\beta}}{SE(\hat{\beta})}$
\item[]
\item p-value: Probability of seeing this (or more extreme) result if $H_0$ is true
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{What p-values tell us (and don't)}
\centering

\begin{itemize}[<+->]
\item p-value \textbf{is}: probability of data given null hypothesis
\item p-value \textbf{is not}: probability that the null is true
\item[]
\item Small p-value: data is surprising if $H_0$ is true
\item Large p-value: data is compatible with $H_0$
\item[]
\item But $H_0$ compatible doesn't mean $H_0$ is true!
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Statistical vs. practical significance}
\centering

\begin{itemize}[<+->]
\item Statistical significance: ``Is the effect different from zero?''
\item Practical significance: ``Is the effect large enough to matter?''
\item[]
\item With enough data, tiny effects become ``significant''
\item A large sample can detect effects too small to care about
\item[]
\item Always consider the \textbf{size} of the effect, not just significance
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Heteroskedasticity and robust standard errors}
\centering

\begin{itemize}[<+->]
\item OLS assumes constant error variance (homoskedasticity)
\item Often violated in practice
\item[]
\item Consequence: Standard errors may be wrong
\item Coefficients are still unbiased, but inference is off
\item[]
\item Solution: Use \textbf{robust standard errors} (HC1, HC2, etc.)
\item Always report robust SEs unless you have good reason not to
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ====================================================
\section{Presenting Results}
% ====================================================

% ----------------------------------------------------
\begin{transitionframe}
Presenting Results
\end{transitionframe}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Principles for presenting regression results}
\centering

\begin{itemize}[<+->]
\item Focus on \textbf{substance}, not just coefficients
\item Show the \textbf{uncertainty} in your estimates
\item Use \textbf{visualizations} when possible
\item Help readers understand the \textbf{size} of effects
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Moving beyond coefficient tables}
\centering

\begin{itemize}[<+->]
\item Tables are useful but hard to interpret
\item[]
\item Better approaches:
  \begin{itemize}
  \item Predicted values at meaningful scenarios
  \item Marginal effects with confidence intervals
  \item First differences (effect of moving from low to high)
  \end{itemize}
\item[]
\item The \texttt{marginaleffects} package in R is excellent for this
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Quantities of interest}
\centering

\begin{itemize}[<+->]
\item \textbf{Predicted values}: $\hat{Y}$ at specific $X$ values
  \begin{itemize}
  \item ``People with college education earn, on average, \$X''
  \end{itemize}
\item[]
\item \textbf{Marginal effects}: Change in $\hat{Y}$ for change in $X$
  \begin{itemize}
  \item ``Each additional year of education increases income by \$Y''
  \end{itemize}
\item[]
\item \textbf{First differences}: $\hat{Y}|_{X_{high}} - \hat{Y}|_{X_{low}}$
  \begin{itemize}
  \item ``Going from high school to college increases income by \$Z''
  \end{itemize}
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Simulation for uncertainty}
\centering

\begin{itemize}[<+->]
\item Coefficients have a sampling distribution
\item We can simulate from this distribution
\item[]
\item Steps:
  \begin{itemize}
  \item Draw from $\hat{\beta} \sim N(\hat{\beta}, V(\hat{\beta}))$
  \item Calculate quantity of interest for each draw
  \item Report mean and percentiles (95\% CI)
  \end{itemize}
\item[]
\item Propagates uncertainty properly through calculations
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{Summary: Key takeaways}
\centering

\begin{itemize}[<+->]
\item Interactions model conditional relationships
\item Non-linear terms allow curved relationships
\item Standard errors measure estimation precision
\item p-values are about data, not hypothesis probability
\item Present results as interpretable quantities, not just coefficients
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{For next week}
\centering

\begin{itemize}
\item Read King et al. (2000), ``Making the most of statistical analyses''
\item Read Brambor et al. (2006), ``Understanding Interaction Models''
\item Check out \href{https://marginaleffects.com}{marginaleffects.com}
\item Complete Problem Set 2
\item[]
\item Next session: Model interpretation and diagnostics
\end{itemize}

\end{frame}
% ----------------------------------------------------
\note{}

% ----------------------------------------------------
\begin{frame}
\frametitle{}
\centering

Questions?

\end{frame}
% ----------------------------------------------------
\note{}
